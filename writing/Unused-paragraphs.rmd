---
title: "Text to use elsewhere"
author: "Anton Ohsson Collentine"
date: "April 24, 2017"
output: html_document
bibliography: ../bibliography/bibliography-FYP.bib
csl: ../bibliography/apa.csl
---
<!-- Perhaps put part in discussion. Including a part about the evidential value ofmarginal significant results (+ simulation?)?-->  Consistent use of p-values can be challenging, yet inconsistency increases the risk of false positives in science. The p-value is defined as the probability of the observed data assuming the null hypothesis is true. That many people find the usage of Null Hypothesis Testing (NHST) and its accompanying p-values confusing, however, is unsurprising. The current adoption dominant in psychology is a mish-mash of Fisher's and Neyman-Pearson's ideas about hypothesis testing [@10.1016/j.socec.2004.09.033], an integration which would have been rejected by both sides. Fisher, from different periods in his life, is responsible both for the idea that all results above a conventional treshold (p = .05) should be summarily rejected as non-significant, and the idea that the exact p-value should be reported and allowed to speak for itself as per its definition without a set cutoff point denoting 'significance'. To this has been added Neyman-Pearson's ideas of longterm error rates (alpha and beta) and power (1-beta), which were meant to be used in the comparison of two competing hypotheses and with a pre-defined alpha value. The conflation of these different decision rules naturally causes confusion, yet in the end what is important to science is not the particular decision rule chosen, but rather the minimization of false positives [see e.g. @10.1371/journal.pmed.0020124] and false negatives [@10.1525/collabra.71]. A common practice which inflates the risk of false positives is the alteration of analytic decisions upon seeing the data [@10.1177/0956797611417632]. One of a multitude of such so called Questionable Research Practices (QRPs) is the post-hoc decision to change what decision rule one uses, or how strictly it is applied [@10.3389/fpsyg.2016.01832]. Unfortunately, the conflation of decision rules and the perceived importance of achieving 'significant' results, creates uncertainty for many researchers when a result is close to, but does not cross below, the conventional p-value treshold (.05). Considering such a result as 'marginally significant' is therefore not uncommon practice. However, since most psychological researchers are likely to use a pre-defined alpha level, typically set to Fisher's conventional 5% level, such practice is an example of changing an analytic decision upon seeing the data. The severity of this practice may depend on to what extent the decision rule has been altered, i.e. the researcher's interpretation of the results, yet overall it appears likely that the usage of 'marginal significance' will increase the risk of false positives. 
