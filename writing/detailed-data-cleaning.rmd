---
title: "Detailed data cleaning"
author: "Anton Ohlsson Collentine"
date: "5 maj 2017"
output: html_document
---

Extracted data was cleaned, subfield categories added, and a subsample for testing and pre-registration purposes created, see figure 1 for an overview of this process. For each article, the following information was extracted: Digital Object Identifier(where available), raw text of the p-values (_e.g._ "$p=.048$"), sign of the p-value comparison ('>','<' or '='), the p-value itself, and finally the 200 characters immediately before the reported p-value, and the 200 characters immediately following. This data was collated into one large dataset containing 790,206 entries. From this dataset we removed a small number of entries (51) lacking identifying information, and all rows where the p-values were not numerical (_e.g._ equal to '.'), due to misreporting or extraction-failure. Note that this does not include p-values that were misreported as too high (_e.g._ $p = 1.0$ instead of $p = .10$) or as possible p-values (_e.g._ $p = .99$ instead of $p = .099$). However, according to @10.3758/s13428-015-0664-2, misreporting occurs for only about .97% of nonsignificant p-values (defined as $p > .05$). The values of interest here were those p-values most likely to be reported as marginally significant, and the dataset was next limited to only inlcude these ($.05 < p <= .1$). According to @10.3758/s13428-015-0664-2 the vast majority of researchers (94.3%) consider $p = .05$ as significant, and as such we limited our sample to $p > .05$. In addition, @10.1177/0956797616645672, in an subanalysis of the distribution of p-values, report a sharp drop in p-values reported as marginally significant above $p = .1$, and we therefore used as an upper limit $p <= .1$. This resulted in a finished datset of 50,891 p-values, originating from 19,875 articles. Approximately 6.4% of all extracted p-values were thus between .05 and .1. Subsequently, missing metadata for 844 entries was retrieved from Crossref. Following this, journal names were standardized for all entries, with older journals names updated to their current (2017) APA-names, before adding subfield categories for each journal. Our categorization of journals into different subfields is taken from the APA's ([www.apa.org](www.apa.org)) divison of their journals into 'topics'. Note, however, that the APA allows the possibility of a journal belonging to several subfields. A full list of which journals were included in which subfields can be found in appendix {1}. Finally, a stratified random sample (by journal) was taken from this finished dataset, containing approximately 6% of entries from each journal. Five journals were not included in this test-sample due to consisting of too few entries (4 or fewer). 
