---
title: "Introduction FYP"
author: "Anton Ohlsson Collentine"
date: "14 april 2017"
output: html_document
---
Consistent use of p-values can be challenging, yet inconsistency increases the risk of false positives in science. <!-- The p-value is defined as the probability of the observed data assuming the null hypothesis is true. That many people find the usage of Null Hypothesis Testing (NHST) and its accompanying p-values confusing, however, is unsurprising. The current adoption dominant in psychology is a mish-mash of Fisher's and Neyman-Pearson's ideas about hypothesis testing (Gigerenzer, 2004), an integration which would have been rejected by both sides. Fisher, from different periods in his life, is responsible both for the idea that all results above a conventional treshold (p = .05) should be summarily rejected as non-significant, and the idea that the exact p-value should be reported and allowed to speak for itself as per its definition without a set cutoff point denoting 'significance'. To this has been added Neyman-Pearson's ideas of longterm error rates (alpha and beta) and power (1-beta), which were meant to be used in the comparison of two competing hypotheses and with a pre-defined alpha value. The conflation of these different decision rules naturally causes confusion, yet in the end what is important to science is not the particular decision rule chosen, but rather the minimization of false positives (see e.g. Ioannidis, 2005) and false negatives (Hartgerink, Wicherts, & van Assen, 2017). A common practice which inflates the risk of false positives is the alteration of analytic decisions upon seeing the data (Simmons, Nelson, & Simonsohn, 2011). One of a multitude of such so called Questionable Research Practices (QRPs) is the post-hoc decision to change what decision rule one uses, or how strictly it is applied (Wicherts et al, 2016). Unfortunately, the conflation of decision rules and the perceived importance of achieving 'significant' results, creates uncertainty for many researchers when a result is close to, but does not cross below, the conventional p-value treshold (.05). Considering such a result as 'marginally significant' is therefore not uncommon practice. However, since most psychological researchers are likely to use a pre-defined alpha level, typically set to Fisher's conventional 5% level, such practice is an example of changing an analytic decision upon seeing the data. The severity of this practice may depend on to what extent the decision rule has been altered, i.e. the researcher's interpretation of the results, yet overall it appears likely that the usage of 'marginal significance' will increase the risk of false positives. -->

Pritschet, Powell and Horne (2016) have examined the incidence of researchers interpreting results as 'marginally' significant in psychology, and concluded that the usage is becoming more prevalent. <!-- The researchers looked at the frequency with which researchers report results as 'marginally significant' or 'approaching significance' in a snapshot of articles in psychological journals. This snapshot consisted of all articles from the journals 'Cognitive Psychology', 'Developmental Psychology', and the 'Journal of Personality and Social Psychology' (JPSP) in the years 1970, 1980, 1990, 2000, and 2010. In total 1,535 articles were extracted, meant to "represent three major subfields of psychology: cognitive, developmental, and social" (p. 1037) over time. Pritschet et al concluded that the proportion of articles reporting results as marginally significant (or variations thereof) has risen considerably in all three journals over the time period 1970 to 2010. Moreover, articles published in JPSP (representing social psychology) were the most likely to report marginally significant results. -->

The outcome variable of Pritschet et al (2016) is unfortunately flawed, casting the researchers' claims into doubt. <!-- The outcome variable used was the proportion of papers reporting one or more results as marginally significant. However, this outcome measure does not take into account the fact that the total number of reported p-values per journal article has increased over the years (Nuijten et al, 2016). Consequently, even if the reality is that relatively fewer p-values are reported as marginally significant, or if the proportions stay the same, we would see an increase in the proportion of papers that contain at least one marginally significant result. Similarly, if articles in a certain journal on average report more p-values, more papers in that journal will report at least one marginally significant result. Articles in JPSP report almost twice as many p-values as those in 'Developmental Psychology' (Nuijten et al, 2016) , though data is not available on 'Cognitive Psychology'. Thus, using as an outcome variable the proportion of papers reporting at least one result as marginally significant provides little to no information on researchers' usage of the concept of marginal significance, neither over time nor across journals. -->

Using a relatively limited sample of journals and years might lead to sampling bias. <!-- While the number of articles examined (1,535) might seem impressive, Nuijten et al (2016) in their research extracted 5,108 articles just from the JPSP, and that despite looking at a shorter time-span (1985-2013) than Pritschet et al (2016). In this limited snapshot, it is possible that editorial policies regarding marginal significance differ from the chosen journals and the population in general, or that the limited number of years leads to spurious results. For example, Lakens (2015), who looked at p-values over the period 1990-2013, found a relative decrease in p-values just above significance (.051 -.059), which might imply that researchers (or editors) find it increasingly important to report only conventionally significant results. A large sample of articles would thus be preferable to examine the prevalence of 'marginally significant' reporting practices in psychology. -->

The purpose of the current study is to investigate how common the practice of reporting results as marginally significant is in psychology, and if it differs over time. <!-- To this end, we looked at reported p-values just above the conventional alpha-level (.05 < p < .1) and analyzed what proportion of such values were considered to be marginally significant. In preparation for this analysis we extracted data from 36,662 articles pertaining to different journals of the American Psychological Association (APA) for all the years between 1985-2016. -->

#References
Gigerenzer, G. (2004). Mindless statistics. The Journal of Socio-Economics, 33(5), 587-606.

Hartgerink, C. H., Wicherts, J. M., & van Assen, M. A. L. M. (2017). Too Good to be False: Nonsignificant Results Revisited. Collabra: Psychology, 3(1).

Ioannidis, J. P. (2005). Why most published research findings are false. PLos med, 2(8), e124.

Lakens, D. (2015). On the challenges of drawing conclusions from p-values just below 0.05. PeerJ, 3, e1142. https://doi.org/10.7717/peerj.1142

Nuijten, M. B., Hartgerink, C. H., van Assen, M. A., Epskamp, S., & Wicherts, J. M. (2016). The prevalence of statistical reporting errors in psychology (1985-2013). Behavior research methods, 48(4), 1205-1226. https://doi.org/10.3758/s13428-015-0664-2

Pritschet, L., Powell, D., & Horne, Z. (2016). Marginally significant effects as evidence for hypotheses: Changing attitudes over four decades. Psychological science, 27(7), 1036-1042. https://doi.org/ 10.1177/0956797616645672

Wicherts, J. M., Veldkamp, C. L., Augusteijn, H. E., Bakker, M., van Aert, R. C., & Van Assen, M. A. (2016). Degrees of Freedom in Planning, Running, Analyzing, and Reporting Psychological Studies: A Checklist to Avoid p-Hacking. Frontiers in Psychology, 7.

