---
bibliography: ../bibliography/bibliography-marginal.bib
csl: ../bibliography/apa.csl
output: word_document
---
```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=6, fig.height=4, echo=FALSE, warning=FALSE, message=FALSE)
```
```{r r_objects}
df <- read.csv("../data/final_df_dataset.csv", stringsAsFactors = FALSE)

# drop redundant variables and reorder columns to facilitate aggregation
df <- df[, !(names(df) %in% c("Core.of.Psychology", "journal", "DOI", "Statistic", "id"))]
df <- df[, c(3:11, 1, 2)]

#Aggregate overall

df.sum <- aggregate(df2 ~ year, data = df, FUN = median)

df.sum$source <- "All APA Journals"

#By subfield

df.collect <- vector("list", length = 9)

for(i in 1:9) { #loop over all subfields
  df.collect[[i]] <- aggregate(df2 ~ year, data = df[df[[i]] == 1,], FUN = median)
}

results.df <- do.call("rbind", df.collect) #Combine into one dataframe

results.df$source <- rep(c("Social", "Experimental", "Clinical", "Developmental", "Educational",
                            "Forensic", "Health","Organizational", "Cognitive"), sapply(df.collect, nrow)) #add data providence to new dataframe

```

Supplemental power analysis

In this supplement we explore power over time in psychology overall and across the nine different disciplines examined in the main manuscript. To do so we use reported degrees of freedom as a proxy for power. As reported in the main manuscript we used 'statcheck' (Epskamp & Nuijten, 2016) to extract APA-formatted statistical results from the dataset of Hartgerink (2016), extracting 521,475 results consisting of _Z_-scores, _F_, _t_, _r_ and Chi-square statistics.

We removed a small number of entries lacking DOI (n = 26, 0.005% of total, see Figure S1), and all _Z_-scores and Chi-square statistics (n = 71,474, 13.76% of total). These latter we excluded as they do not provide sufficient information on sample size. Next, we added meta-data, following the same procedure described in the data preparation section of the main manuscript. Finally, for this dataset as well, we excluded entries unique to the topic 'Core of psychology' (n = 3,382, 0.65% of total), which left us with a final dataset consisting of 446,320 entries, where each entry corresponded to an _r_, _t_ or _F_ statistic.

```{r flowchart, fig.cap="_Figure S1._ Flowchart illustrating the process generating the final degrees of freedom dataset.", echo = FALSE, fig.align='center'}
knitr::include_graphics("../figures/flowchart_df.png", auto_pdf = TRUE)
```

For our analysis we aggregated the extracted statistics by year and calculated the median degrees of freedom for each year and discipline (Figure S2). In the case of _F_-statistics we used the denominator degrees of freedom. We also report ten simple linear regression using least squares, one for each discipline and overall, with the median degrees of freedom as the outcome variable and year (1985 - 2016) as the independent variable. For plotting, but not for the linear regressions, the median degrees of freedom were averaged over two years for the disciplines, due to their large variation from year to year.

```{r df_plot, fig.cap="_Figure S2._ Median degrees of freedom across disciplines over time. Values for disciplines, but not overall, are averages over two years.", echo = FALSE, fig.align='center'}
knitr::include_graphics("../figures/df_plot.png", auto_pdf = TRUE)
```

As can be seen in Figure S2, overall there is a small increase in the median number of degrees of freedom over time (_b_ = 0.4), and the degrees of freedom increased in all disciplines over time, the most in Developmental (_b_ = 1.99) and the least in Cognitive (_b_ = 0.16). Using degrees of freedom as a proxy for power, there thus appears to be a weak tendency for more power over time in psychology, assuming similar true effect size distributions, designs, etc. over time and disciplines. Given these rather strong assumptions, this increase in power should lead to more _p_-values below .05 and fewer in the .05 - .1 range. Considering one of the dependent variables of the main manuscript, the percentage of articles containing at least one _p_-values between .05 and .1 reported as marginally significant, we would thus expect this percentage to decrease over time. This appears to be the case for some disciplines (see Figure 3 of main manuscript), although generally the percentage of articles reporting at least one 'marginally significant' result was largely stable over time, with the largest slope coefficient being positive (Experimental psychology; _b_ = 0.15). 

The apparently weak association between power and the article level outcome variable could be due to the increase in power over time being relatively small, due to the imperfections and assumptions required when using the reported degrees of freedom as a proxy for power, or other factors which influence _p_-value distributions, such as _p_-hacking and publication bias. As stated in the main manuscript, power has no effect on our other outcome variable, the percentage of _p_-values between .05 and .1 reported as marginally significant.







