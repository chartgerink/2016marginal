---
output: html_document
bibliography: ../bibliography/bibliography-marginal.bib
csl: ../bibliography/apa.csl
figs: ../figures
---
```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=6, fig.height=4,
                      echo=FALSE, warning=FALSE, message=FALSE)
```
```{r r_objects}
marginal <- readRDS("./marginal_rmarkdown_objects.RData")
```
```{r table_1, results="asis"}
library(pander)
panderOptions("keep.line.breaks", TRUE)
colnames(marginal$tablerep) <- c("Journal", "Year", "Pritschet et al.\\\nArticles", "Pritschet et al.\\\nMarg. Sig. (%)", 
                                 "Replication\\\nArticles", "Replication\\\nP-values per article", 
                                 "Replication\\\n.05 < p <= .1 (%)", "Replication\\\nMarg. sig. (%)")
pandoc.table(marginal$tablerep, caption = "_Table 1._", split.table = Inf)
```

##Method
@10.3390/data1030014 previously downloaded 74,489 articles published between 1985 and 2016 from 74 APA-journals (`r signif(100*(74/93), digits = 2)`% of extant APA journals 2017), which we used as a basis for the current project. Table 1 compares the data extracted by @10.1177/0956797616645672 and us on those points where such a comparison is possible. @10.3390/data1030014 downloaded articles in in HTML format, meaning that the time span for data from any given journal depended on from what year articles in each journal were available in HTML format. In addition, though @10.3390/data1030014 collected data from several different publishers, we only used data from journals belonging to the APA since these were already divided into subfields, or 'topics' @APAjournals. We converted HTML articles to raw text format using the python tool 'html2text', and subsequently extracted the following information from each article using regular expressions: DOI (when available), raw text of the p-values (_e.g._ "$p=.048$"), sign of the p-value comparison ('>','<' or '='), the p-value itself, and finally the 200 characters immediately before the reported p-value, and the 200 characters immediately following. To determine whether a result was reported as marginally significant, we searched the 200 characters preceding and succeeding a given p-value for strings containing "marginal" and "approach", and considered the p-value to be reported as marginally significant if either of those strings was found. This data we collated into one large dataset containing `r marginal$entries.original` entries. 

```{r flowchart, fig.cap="_Figure 1._ Flowchart illustrating the data cleaning process.", out.width="60%", out.height="60%", fig.align="center", echo = FALSE}
knitr::include_graphics("../figures/flowchart_marginal.png", auto_pdf = TRUE)
```

###Data
Extracted data was cleaned, subfield categories added, and a subsample for testing and pre-registration purposes created (figure 1). From the articles provided by @10.3390/data1030014 `r marginal$entries.original` p-values were extracted. From this dataset we removed a small number of entries (n = `r marginal$nodoi`, `r marginal$nodoipercent`% of total) lacking DOI and other identifying information, and all rows (n = `r marginal$badp`, `r marginal$badppercent`% of total) where the p-values were not numerical (_e.g._ equal to '.'). This does not include p-values that were misreported as too high (_e.g._ $p = 1.2$ instead of $p = .12$) or as possible p-values (_e.g._ $p = .99$ instead of $p = .099$). However, according to @10.3758/s13428-015-0664-2, misreporting occurs for only about .97% of nonsignificant p-values (defined as $p > .05$). Missing metadata (years and journal name) for 12775 (1.62%) entries was retrieved from Crossref using the r-package 'rcrossref', and journal names were standardized for all entries, with older journal names updated to their current (2017) APA-names as per Appendix A, before adding subfield categories for each journal. A journal may publish on several topics, and a full list of which journals were included in which subfields can be found in appendix A. Finally, the dataset was limited to p-values $.05 < p <= .1$, resulting in a final sample consisting of `r marginal$entries.final` (`r marginal$entries.finalpercent`%) p-values, originating from `r marginal$articles.final` (`r marginal$articles.finalpercent`%) articles in `r marginal$journals.final` different journals. One journal, the '`r marginal$difjournals`', did not contain any p-values between .05 and .1. A stratified random sample (by journal) of about 6% was taken from the finished dataset for testing code for pre-registration. Table 2 gives an overview of the data extracted, presented by subfield.
```{r table_2, results="asis"}
colnames(marginal$tablesubfields) <- c("Field", "Journals", "Articles", "P-values", "P-values per article",
                                 ".05 < p <= .1 (%)", "Marg. sig. (%)")
pandoc.table(marginal$tablesubfields, caption = "_Table 1._", split.table = Inf)
```

###Analysis
We describe the percentage of p-values ($.05 < p <= .1$) reported as marginally significant over the years in a population of APA journals, overall and by subfields. Due to the size and non-random nature of our sample we consider it our population of interest and report only descriptive statistics. This precludes the need to consider dependencies, and as such we disaggregated all extracted p-values before analyzing them by subfield and overall. To facilitate interpretation we estimated linear trends, and report regression coefficients as well as proportion of variance explained. All analytic code was first run on a test-sample consisting of 6% of the p-values and then pre-registered.

##References
