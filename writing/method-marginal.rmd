---
output: word_document
bibliography: ../bibliography/bibliography-marginal.bib
csl: ../bibliography/apa.csl
figures: yes
---
```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=6, fig.height=4, echo=FALSE, warning=FALSE, message=FALSE)
```
```{r r_objects}
marginal <- readRDS("./marginal_rmarkdown_objects.RData")
```

##Method
###Data
We reused raw text data from @10.3390/data1030014, which consisted of articles downloaded from journals belonging to the APA. @10.3390/data1030014 downloaded 74,489 articles published between 1985 and 2016 in 74 different APA journals (`r signif(100*(74/93), digits = 2)`% of currently existing APA journals). As @10.3390/data1030014 only downloaded articles in HTML format, the time span for each journal depends on the year articles became available in HTML format. Moreover, though @10.3390/data1030014 collected data from several different publishers, we limited ourselves to data from journals belonging to the APA, since the APA divides their journals into topics [@APAjournals]. We use these topics to represent nine psychological subfields: "_Basic / Experimental Psychology_" (experimental psychology), "_Clinical Psychology_" (clinical psychology), "_Developmental Psychology_" (developmental psychology), "_Educational Psychology, School Psychology & Training_" (educational psychology), "_Forensic Psychology_" (forensic psychology), "_Health Psychology & Medicine_" (health psychology), "_Industrial/Organizational Psychology & Management_" (organizational psychology), "_Neuroscience & Cognition_" (cognitive psychology), and "_Social Psychology & Social Processes_" (social psychology). The topic "_Core of Psychology_" [@APAjournals] consists of journals that publish on general or interdisciplinary psychology, and we therefore do not consider it a psychological subfield in the current paper. In addition, per the APA's categorization a journal may publish in several topics. See appendix A for a detailed breakdown of journals (not) included in the sample, and their division into topics and subfields. 

We converted downloaded HTML articles into plain text, and extracted p-values with surrounding text. We converted HTML articles to raw text format using the python tool 'html2text', and subsequently extracted the following information from each article using regular expressions: DOI (when available), raw text of the p-values (_e.g._ "$p=.048$"), sign of the p-value comparison ('>','<' or '='), the p-value itself, and finally the 200 characters immediately before the reported p-value, and the 200 characters immediately following. This data we collated into one large dataset containing `r marginal$entries.original` entries. All code for this project is available at osf.io/XXXX and was preregistered at osf.io/XXXX. 

###Data cleaning
Extracted data was cleaned, subfield categories added, and a subsample for testing and pre-registration purposes created (figure 1). From the articles provided by @10.3390/data1030014 we extracted`r marginal$entries.original` p-values. From this dataset we removed a small number of entries (n = `r marginal$nodoi`, `r marginal$nodoipercent`% of total) lacking DOI (and journal name/year), and all rows (n = `r marginal$badp`, `r marginal$badppercent`% of total) where the p-values were not numerical (_e.g._ equal to '.'). This does not include p-values that were misreported as too high (_e.g._ $p = 1.2$ instead of $p = .12$) or as possible p-values (_e.g._ $p = .99$ instead of $p = .099$). However, according to @10.3758/s13428-015-0664-2, misreporting occurs for only about .97% of nonsignificant p-values (defined as $p > .05$). We retrieved missing metadata (years and journal name) for `r marginal$mis.meta` (`r marginal$mis.metapercent`% of total) entries from Crossref using the r-package 'rcrossref'[@rcrossref], and standardized journal names for all entries, with older journal names updated to their current (2017) APA-names as per Appendix A, before adding subfield categories for each journal. Finally, we limited the dataset to p-values $.05 < p \leq.1%$, resulting in a final sample consisting of `r marginal$entries.final` (`r marginal$entries.finalpercent`% of total) p-values, originating from `r marginal$articles.final` (`r marginal$articles.finalpercent`%) articles in `r marginal$journals.final` different journals. One journal, the '`r marginal$difjournals`', did not contain any p-values between .05 and .1. A stratified random sample (by journal) of about 6% was taken from the finished dataset for testing code for pre-registration. 

```{r flowchart, fig.cap="_Figure 1._ Flowchart illustrating the data cleaning process.", echo = FALSE, fig.align='center'}
knitr::include_graphics("../figures/flowchart_marginal.png", auto_pdf = TRUE)
```

Table 1 summarizes our data per subfield. To determine whether a result was reported as marginally significant, we searched the 200 characters preceding a given p-value for the strings "margin\*" and "approach\*" using regular expressions, and considered the p-value to be reported as marginally significant if either of those strings was found. We searched the characters preceding the _p_-value as interpretation of a _p_-value typically comes before the reported statistic. Including the 200 characters succeeding the _p_-value increased the percentage of results considered marginally significant by `r marginal$marg.diff.overall`% overall and with `r marginal$marg.diff.max`% for the subfield with the largest increase (social psychology).

```{r table_1}
library(knitr)
kable(marginal$table1, caption = "Table 1. Specification for different psychological subfields of the number of APA journals included, the number of articles downloaded, the number of _p_-values extracted (and per article), the number of _p_-values betwen .05 and .1 (and per article), and the percentage of _p_-values betwen .05 and .1 that were reported as marginally significant", align = "l", col.names = c("Field", "Journals", "Articles", "P-values (per article)", ".05 < p <= .1 (per article)", "Marg. Sig. In %"))
```

Table 2 compares our data with that of @10.1177/0956797616645672 for the two APA-journals that they included in their dataset: The 'Journal of Personality and Social Psychology' (JPSP) and 'Developmental Psychology' (DP). As @10.1177/0956797616645672 only searched for results reported as marginally significant, their dataset does not include the overall number of _p_-values nor the overall number of _p_-values that fall between .05 and .1.

```{r table_2}
library(knitr)
kable(marginal$table2, caption = "Table 2. Comparison between Pritschet et al (2016) and the current paper of the time span of downloaded articles, the number of articles downloaded, the number of _p_-values extracted (and per article), the number of _p_-values betwen .05 and .1 (and per article), and the percentage of _p_-values  that were reported as marginally significant for the two journals 'Journal of Personality and Social Psychology' (JPSP) and 'Developmental Psychology' (DP)", align = "l", col.names = c("Journal", "Time Span", "Articles", "P-values (per article)", ".05 < p <= .1 (per article)", "Marg. Sig. In %"))
```


###Analysis
We describe the percentage of p-values ($.05 < p <= .1$) reported as marginally significant over the years in a population of APA journals, overall and by subfields. Due to the size and non-random nature of our sample we consider it our population of interest and report only descriptive statistics. This precludes the need to consider dependencies, and as such we disaggregated all extracted p-values before analyzing them by subfield and overall. To facilitate interpretation we estimated linear trends, and report regression coefficients as well as proportion of variance explained. All analytic code was first run on a test-sample consisting of 6% of the p-values and then pre-registered at osf.io/XXXX.

##References
