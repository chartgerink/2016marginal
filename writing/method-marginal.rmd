---
output: html_document
bibliography: ../bibliography/bibliography-FYP.bib
csl: ../bibliography/apa.csl
figs: ../figures
---
```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=6, fig.height=4,
                      echo=FALSE, warning=FALSE, message=FALSE)
```
```{r r_objects}
marginal <- readRDS("./marginal_rmarkdown_objects.RData")
```
```{r table_1, results="asis"}
library(pander)
panderOptions("keep.line.breaks", TRUE)
colnames(marginal$tablerep) <- c("Journal", "Year", "Pritschet et al.\\\nArticles", "Pritschet et al.\\\nMarg. Sig. (%)", 
                                 "Replication\\\nArticles", "Replication\\\nP-values per article", 
                                 "Replication\\\n.05 < p <= .1 (%)", "Replication\\\nMarg. sig. (%)")
pandoc.table(marginal$tablerep, caption = "_Table 1._", split.table = Inf)
```

##Method
@10.3390/data1030014 previously extracted 74,489 articles published between 1985 and 2016 from 74 APA-journals (constituting `r signif(100*(74/93), digits = 2)`% of extant APA journals 2017), which were used as a basis for the current project. Table 1 compares the data extracted by @10.1177/0956797616645672 and us on those points where such a comparison is possible. Articles were downloaded in HTML format, meaning that the time span for data from any given journal depended on from what year articles in each journal were available in HTML format. In addition, though @10.3390/data1030014 collected data from several different publishers, only data from journals belonging to the APA was used in the current project due to ready access to this particular data [not really happy with this sentence, but feel that it needs to be commented and can't come up with a better way to express it]. The HTML articles were converted to raw text format using the python tool 'html2text', and the following information subsequently extracted from each article using regular expressions: Digital Object Identifier (where available), raw text of the p-values (_e.g._ "$p=.048$"), sign of the p-value comparison ('>','<' or '='), the p-value itself, and finally the 200 characters immediately before the reported p-value, and the 200 characters immediately following. To determine whether a result was reported as marginally significant, we searched the 200 characters preceding and succeeding a given p-value for strings containing "marginal" and "approach", and considered the p-value to be reported as marginally significant if either of those strings was found. This data was collated into one large dataset containing `r marginal$entries.original` entries. 

```{r flowchart, fig.cap="_Figure 1._ Flowchart illustrating the data cleaning process.", out.width="60%", out.height="60%", fig.align="center", echo = FALSE}
knitr::include_graphics("../figures/flowchart_marginal.png", auto_pdf = TRUE)
```

###Data
Extracted data was cleaned, subfield categories added, and a subsample for testing and pre-registration purposes created (figure 1). From the articles provided by @10.3390/data1030014 `r marginal$entries.original` p-values were extracted. From this dataset we removed a small number of entries (n = `r marginal$nodoi`, `r marginal$nodoipercent`% of total) lacking DOI and other identifying information, and all rows (n = `r marginal$badp`, `r marginal$badppercent`% of total) where the p-values were not numerical (_e.g._ equal to '.'). This does not include p-values that were misreported as too high (_e.g._ $p = 1.2$ instead of $p = .12$) or as possible p-values (_e.g._ $p = .99$ instead of $p = .099$), however, according to @10.3758/s13428-015-0664-2, misreporting occurs for only about .97% of the nonsignificant p-values (defined as $p > .05$) which are of interest here. Missing metadata (years and journal name) for 12775 (1.62%) entries was retrieved from Crossref using the r-package 'rcrossref', and journal names were standardized for all entries, with older journal names updated to their current (2017) APA-names as per Appendix A, before adding subfield categories for each journal. Our categorization of journals into different subfields is taken from the APA's ([www.apa.org](www.apa.org)) divison of their journals into 'topics'. A journal may publish on several topics; a full list of which journals were included in which subfields can be found in appendix A. Finally, the dataset was limited to p-values $.05 < p <= .1$, resulting in a final sample consisting of `r marginal$entries.final` (`r marginal$entries.finalpercent`%) p-values, originating from `r marginal$articles.final` (`r marginal$articles.finalpercent`%) articles in `r marginal$journals.final` different journals. One journal, the '`r marginal$difjournals`', did not contain any p-values between .05 and .1. A stratified random sample (by journal) of about 6% was taken from the finished dataset for testing code for pre-registration. Table 2 gives an overview of the data extracted, presented by subfield.
```{r table_2, results="asis"}
colnames(marginal$tablesubfields) <- c("Field", "Journals", "Articles", "P-values", "P-values per article",
                                 ".05 < p <= .1 (%)", "Marg. sig. (%)")
pandoc.table(marginal$tablesubfields, caption = "_Table 1._", split.table = Inf)
```

###Analysis
We describe the percentage of p-values ($.05 < p <= .1$) reported as marginally significant in a population of APA journals, overall and by subfields. Due to the size and non-random nature of our sample we consider it our population of interest and report only descriptive statistics. This precludes the need to consider dependencies, and as such we disaggregated all extracted p-values before analyzing them by subfield and overall. To facilitate interpretation we estimated linear trends, and report regression coefficients as well as proportion of variance explained. All analytic code was first run on a test-sample consisting of 6% of the p-values and then pre-registered.

##References
