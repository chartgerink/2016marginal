---
output: word_document
bibliography: ../bibliography/bibliography-marginal.bib
csl: ../bibliography/apa.csl
figures: yes
---
```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=6, fig.height=4, echo=FALSE, warning=FALSE, message=FALSE)
```
```{r r_objects}
marginal <- readRDS("./marginal_rmarkdown_objects.RData")
```

#Method
All code and data for this project is available at osf.io/28gxz (preserved at [XXXX]). We refer to the relevant code files on OSF using brackets and links in the sections below, i.e. (osf.io/file/). We ran all analyses using R version 3.4.1 [@Rref]. 

##Data
We reused downloaded articles from @10.3390/data1030014, consisting of 74,489 articles published between 1985 and 2016 in 74 APA journals (`r signif(100*(74/93), digits = 2)`% of currently existing APA journals). We limited ourselves to data from journals belonging to the APA, since the APA divides their journals into topics [@APAjournals]. We use these topics to represent nine psychological disciplines: "_Basic / Experimental Psychology_" (experimental psychology), "_Clinical Psychology_" (clinical psychology), "_Developmental Psychology_" (developmental psychology), "_Educational Psychology, School Psychology & Training_" (educational psychology), "_Forensic Psychology_" (forensic psychology), "_Health Psychology & Medicine_" (health psychology), "_Industrial/Organizational Psychology & Management_" (organizational psychology), "_Neuroscience & Cognition_" (cognitive psychology), and "_Social Psychology & Social Processes_" (social psychology). The topic "_Core of Psychology_" [@APAjournals] consists of journals that publish on general or  interdisciplinary psychology, and we therefore do not consider it a discipline in psychology and exclude entries unique to it from our final dataset. Four journals and `r marginal$difarticles` articles were unique to this category. See Appendix A for a detailed summary of journals (not) included in our sample, and their division into topics and disciplines. 

As @10.3390/data1030014 only downloaded articles in HTML format, the time span for each journal depends on the year articles became available in HTML format. We ordered downloaded HTML articles into separate folders, and then converted them into raw text using the python tool 'html2text (osf.io/4yqhj/; pypi.python.org/pypi/html2text). Thereupon we attempted to download metadata for all articles (osf.io/f3mga/). We extracted the following information from each article using regular expressions (osf.io/qaw74/): DOI (when available), raw text of the _p_-values (_e.g._ "$p=.048$"), sign of the _p_-value comparison ('>','<' or '='), the _p_-value itself, and the 200 characters preceding the reported _p_-value, and the 200 characters immediately succeeding. We collated these data into one large dataset containing `r marginal$entries.original` entries, with one entry pertaining to results of one p-value (osf.io/f3mga/).

##Data Preparation
We excluded a small number of entries from the extracted data due to misreporting or extraction failure (see Figure 1 for a flowchart). We removed  entries lacking DOI (and journal name/year; n = `r marginal$nodoi`, `r marginal$nodoipercent`% of total), and all entries where the _p_-values were not numerical (_e.g._ equal to "."; n = `r marginal$badp`, `r marginal$badppercent`% of total; osf.io/gzyt9/). _P_-values that were misreported as too high (_e.g._ $p = 1.2$ instead of $p = .12$) were excluded together with all other _p_-values above .1 at a later stage, see below. Note that a few misreported _p_-values will remain in the dataset, those misrepored as _e.g._ $p = .099$ instead of $p = .99$.

Subsequently, we added discipline information to each entry. Before adding this information, we used the r-package 'rcrossref' [@rcrossref] to retriev missing metadata (years and journal name) for all entries lacking such data (n = `r marginal$mis.meta; marginal$mis.metapercent`% of total; osf.io/gzyt9/). We also standardized journal names for all entries, with older journal names updated to their current APA-names (2017, see Appendix A; osf.io/gzyt9/). We then added dummies for each discipline to all entries (osf.io/gzyt9/). 

Finally, we excluded the topic 'core of psychology', all _p_-values outside of the range of .05 - .1 and created a test sample. We excluded `r marginal$unique.core` (`r marginal$unique.corepercent`% of total) entries unique to the topic 'core of psychology' (osf.io/gzyt9/). Limiting the dataset to _p_-values $.05 < p \leq.1%$ then resulted in a final sample consisting of `r marginal$entries.final` (`r marginal$entries.finalpercent`% of total) _p_-values (osf.io/gzyt9/). From the final dataset, we drew a stratified random sample of 6% per journal for testing code used for data analysis (osf.io/y953k/). For our analyses reported below we used the full final dataset, including the test sample data.

```{r flowchart, fig.cap="_Figure 1._ Flowchart illustrating the process generating the test sample and the final dataset.", echo = FALSE, fig.align='center'}
knitr::include_graphics("../figures/flowchart_marginal.png", auto_pdf = TRUE)
```

Table 1 summarizes the data per discipline. As per the APA's categorization, a journal may belong to multiple disciplines (see also Appendix A). A _p_-value in an article is thus part of the _p_-value count for each discipline it belongs to. To determine whether a result was reported as marginally significant, we searched the 200 characters preceding and the 200 characters succeeding a given _p_-value for the expressions "margin\*" and "approach\*" using regular expressions and considered the _p_-value to be reported as marginally significant if either of those expressions was found.

```{r table_1}
library(knitr)
kable(marginal$table1, caption = "Table 1. Specification for different psychological disciplines of the number of APA journals scanned for _p_-values, the number of articles with extracted data, the number of _p_-values extracted (and per article), the number of _p_-values between .05 and .1 (and per article), and the percentage of _p_-values between .05 and .1 that were reported as marginally significant.", align = "l", col.names = c("Discipline", "Journals", "Articles", "_p_-values (per article)", ".05 < p <= .1 (per article)", "Marg. Sig. In %"))
```

Table 2 compares our data with the data provided by @10.1177/0956797616645672 (available at osf.io/92xqk) with respect to the two APA-journals (DP and JPSP) that their article and ours have in common. @10.1177/0956797616645672 concerned themselves only with the binary option of whether an article contained a 'marginally significant' result or not, and consequently each row in their dataset represents a different article. As such, their data does not include the total number  of _p_-values, nor the number of _p_-values between .05 and .1. Moreover, their percentage of 'marginally significant' results is the proportion of articles containing at least one result reported as marginally significant, whereas in the current paper that percentage is the proportion of _p_-values between .05 and .1 reported as marginally significant. 

```{r table_2}
library(knitr)
kable(marginal$table2, caption = "Table 2. Comparison between the data of @10.1177/0956797616645672 and the current paper with respect to the journals 'Journal of Personality and Social Psychology' (JPSP) and 'Developmental Psychology' (DP). Presented are time span of downloaded articles, the number of articles examined, the number of _p_-values extracted (and per article), the number of _p_-values between .05 and .1 (and per article), and the percentage of _p_-values  that were reported as marginally significant. The dataset of @10.1177/0956797616645672 does not include information on the total number of _p_-values nor the number of _p_-values between .05 and .1, resulting in NA-values." , align = "l", col.names = c("Journal", "Time Span", "Articles", "_p_-values (per article)", ".05 < p <= .1 (per article)", "Marg. Sig. In %"))
```

##Analyses
Due to using non-random (only APA-articles available in HTML format at the time of download) dependent samples (many _p_-values are included in multiple disciplines), and dependent observations at the level of the article (multiple 'marginally significant' _p_-values in one article), we present only descriptive statistics and conduct no inferential statistical testing. We describe trends in percentages of all 'marginally significant' results across years and disciplines, and for only JPSP/DP (osf.io/wa62v/). To aid interpretation we estimate 12 simple linear regressions using least squares estimation, one for each discipline, overall and one each for JPSP/DP. We report estimated _b_-values and coefficients of determination. The outcome variable in these regressions is the proportion of _p_-values ($.05 < p \leq.1%$) reported as marginally significant per year in each category. The independent variable is the year (range 1985 - 2016) of publication of the articles from which the _p_-values were extracted. In addition we report averages across the years for each category (osf.io/79t2p/). We will conduct no other confirmatory data analyses. Any additional analyses we will accordingly label as exploratory. 