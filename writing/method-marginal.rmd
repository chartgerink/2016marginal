---
output: html_document
bibliography: ../bibliography/bibliography-marginal.bib
csl: ../bibliography/apa.csl
figures: yes
---
```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=6, fig.height=4, echo=FALSE, warning=FALSE, message=FALSE)
```
```{r r_objects}
marginal <- readRDS("./marginal_rmarkdown_objects.RData")
```

##Method
All code and data for this project is available at osf.io/28gxz (preserved at [XXXX]). Introduction, method section, data, bibliography (up until method section) as well as code for data cleaning, test sample creation, analysis, and creation of in-text values/tables (up until method section) are preregistered at osf.io/XXXX. We refer to the relevant code files in the sections below. Our data were available before pre-registration but, except for a sub-sample for testing code, we withheld it from analysis until after pre-registration.
###Data
We reused downloaded articles from @10.3390/data1030014, consisting of 74,489 articles published between 1985 and 2016 in 74 APA journals (`r signif(100*(74/93), digits = 2)`% of currently existing APA journals). We limited ourselves to data from journals belonging to the APA, since the APA divides their journals into topics [@APAjournals]. We use these topics to represent nine psychological subfields: "_Basic / Experimental Psychology_" (experimental psychology), "_Clinical Psychology_" (clinical psychology), "_Developmental Psychology_" (developmental psychology), "_Educational Psychology, School Psychology & Training_" (educational psychology), "_Forensic Psychology_" (forensic psychology), "_Health Psychology & Medicine_" (health psychology), "_Industrial/Organizational Psychology & Management_" (organizational psychology), "_Neuroscience & Cognition_" (cognitive psychology), and "_Social Psychology & Social Processes_" (social psychology). The topic "_Core of Psychology_" [@APAjournals] consists of journals that publish on general or interdisciplinary psychology, and we therefore do not consider it a psychological subfield and exclude entries unique to it from the current paper. Four journals and `r marginal$difarticles` articles were unique to this category. See Appendix A for a detailed breakdown of journals (not) included in our sample, and their division into topics and subfields. 

As @10.3390/data1030014 only downloaded articles in HTML format, the time span for each journal depends on the year articles became available in HTML format. We ordered (apa-foldering.sh) downloaded HTML articles into separate folders, and then converted them into raw text using the python tool 'html2text (https://pypi.python.org/pypi/html2text). Thereupon we attempted (see readme.md) to download metadata for all articles. We extracted (data-extraction.r) the following information from each article using regular expressions: DOI (when available), raw text of the _p_-values (_e.g._ "$p=.048$"), sign of the _p_-value comparison ('>','<' or '='), the _p_-value itself, and the 200 characters preceding the reported _p_-value, and the 200 characters immediately succeeding. We collated (see readme.md)these data into one large dataset containing `r marginal$entries.original` entries, with one entry corresponding to results of one p-value.

###Data Preparation
We excluded a small number of entries from the extracted data due to misreporting or extraction failure (see Figure 1 for a flowchart). We removed (data-cleaning.r) entries lacking DOI (and journal name/year; n = `r marginal$nodoi`, `r marginal$nodoipercent`% of total), and all entries where the _p_-values were not numerical (_e.g._ equal to "."; n = `r marginal$badp`, `r marginal$badppercent`% of total). _P_-values that were misreported as too high (_e.g._ $p = 1.2$ instead of $p = .12$) were excluded together with all other _p_-values above .1 at a later stage, see below. _P_-values misreported as possible values (_e.g._ $p = .99$ instead of $p = .099$) remain in the dataset. However, according to @10.3758/s13428-015-0664-2 [p. 1212], misreporting occurs for only about .97% of nonsignificant _p_-values (defined as $p > .05$). 

Subsequently, we added (code-data-cleaning.r) subfield categories to each entry. Before adding this information, we retrieved (data-cleaning.r) missing metadata (years and journal name) for all entries lacking such data (n = `r marginal$mis.meta; marginal$mis.metapercent`% of total) using the r-package 'rcrossref' [@rcrossref] We also standardized (code-data-cleaning.r) journal names for all entries, with older journal names updated to their current APA-names (2017, see Appendix A). We then added (data-cleaning.r) dummies for the subfield categories to all entries. 

Finally, we excluded (data-cleaning.r) _p_-values outside of the range of .05 - .1 and created a test sample. Limiting the dataset to _p_-values $.05 < p \leq.1%$, resulted in a final sample consisting of `r marginal$entries.final` (`r marginal$entries.finalpercent`% of total) _p_-values. From the final dataset, we drew (test-sample.r) a stratified (by journal) random sample  of 6% for testing and pre-registering code used for data analysis. For our analyses reported below we used the full final dataset (including the test sample data).

```{r flowchart, fig.cap="_Figure 1._ Flowchart illustrating the process generating the test sample and the final dataset.", echo = FALSE, fig.align='center'}
knitr::include_graphics("../figures/flowchart_marginal.png", auto_pdf = TRUE)
```

Table 1 summarizes the data per subfield. As per the APA's categorization, a journal may belong to multiple subfields (see also Appendix A). A _p_-value in an article is thus part of the _p_-value count for each subfield it belongs to. To determine whether a result was reported as marginally significant, we searched the 200 characters preceding and the 200 characters succeeding a given _p_-value for the expressions "margin\*" and "approach\*" using regular expressions, and considered the _p_-value to be reported as marginally significant if either of those expressions was found.

```{r table_1}
library(knitr)
kable(marginal$table1, caption = "Table 1. Specification for different psychological subfields of the number of APA journals scanned for _p_-values, the number of articles with extracted data, the number of _p_-values extracted (and per article), the number of _p_-values between .05 and .1 (and per article), and the percentage of _p_-values between .05 and .1 that were reported as marginally significant. All values after removal of non-numerical _p_-values and _p_-values in articles without DOI, journal name and year (n = 1,124; 0.14% of total _p_-values).", align = "l", col.names = c("Field", "Journals", "Articles", "_p_-values (per article)", ".05 < p <= .1 (per article)", "Marg. Sig. In %"))
```

Table 2 compares our data with the data provided by @10.1177/0956797616645672 [available at osf.io/92xqk] with respect to the two APA-journals (DP and JPSP) that their article and ours have in common. @10.1177/0956797616645672 were only concerned with the binary option of whether an article contained a 'marginally significant' result or not, and consequently each row in their dataset represent a different article. As such, their data does not include the total number  of _p_-values, nor the number of _p_-values between .05 and .1. Moreover, their percentage of 'marginally significant' results is the proportion of articles containing at least one result reported as marginally significant, whereas in the current paper that percentage is the proportion of _p_-values between .05 and .1 reported as marginally significant. 

```{r table_2}
library(knitr)
kable(marginal$table2, caption = "Table 2. Comparison between the data of @10.1177/0956797616645672 and the current paper with respect to the journals 'Journal of Personality and Social Psychology' (JPSP) and 'Developmental Psychology' (DP). Presented are time span of downloaded articles, the number of articles examined, the number of _p_-values extracted (and per article), the number of _p_-values between .05 and .1 (and per article), and the percentage of _p_-values  that were reported as marginally significant. The dataset of @10.1177/0956797616645672 does not include information on the total number of _p_-values nor the number of _p_-values between .05 and .1, resulting in NA-values." , align = "l", col.names = c("Journal", "Time Span", "Articles", "_p_-values (per article)", ".05 < p <= .1 (per article)", "Marg. Sig. In %"))
```


###Analyses

Due to using a non-random sample (only APA-articles available in HTML format at the time of download), we present only descriptive statistics and conduct no inferential tests. We describe (code-analysis.r) trends in percentages of 'marginally significant' results across years and subfields, and for JPSP/DP. To aid interpretation we estimate 12 simple linear regressions using least squares estimation, one for each subfield, journal and overall. We report estimated _b_-values and coefficients of determination. The dependent variable in these regressions is the proportion of_p_-values ($.05 < p \leq.1%$) reported as marginally significant per year in each category. The independent variable is the year (range 1985 - 2016) of publication of the articles from which the _p_-values were extracted. In addition we report (code-rmarkdown-objects.r) averages across the years for each category. We will conduct no other confirmatory data analyses, and should any additional analyses be conducted we will accordingly label them as exploratory.

We present our results in two steps. First, we present results for the journals JPSP and DP. Here we also highlight the years (1990, 2000, 2010) which our dataset shares with that of @10.1177/0956797616645672 for the two journals. Secondly, we present the results for all included APA-journals taken together, and for the nine psychological subfields previously described (see table 1). We ran all analyses using R version 3.4.1 [@Rref].




##References
